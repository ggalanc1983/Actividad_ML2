{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "912cffc8",
      "metadata": {
        "id": "912cffc8"
      },
      "source": [
        "# Actividad 1 – Machine Learning II (Churn)\n",
        "\n",
        "**Objetivo:** predecir la probabilidad de fuga de clientes (churn) en una empresa de telecomunicaciones mediante el uso de Regresión Logística.\n",
        "\n",
        "Para este propósito, se comparan distintos enfoques de modelamiento:\n",
        "\n",
        "- Un modelo base, construido a partir de variables previamente preprocesadas.\n",
        "\n",
        "- Un modelo con transformaciones polinomiales, de grado 2, aplicadas sobre un conjunto seleccionado de variables numéricas.\n",
        "\n",
        "- Modelos regularizados, que incorporan penalizaciones L2 y L1 (Elastic Net) para controlar la complejidad del modelo.\n",
        "\n",
        "- La evaluación del desempeño se realiza mediante validación cruzada estratificada (k-fold), considerando métricas y herramientas de análisis acordes a un problema de clasificación con posible desbalance de clases, tales como:\n",
        "\n",
        "- Matriz de confusión y métricas asociadas (accuracy, precision, recall y F1).\n",
        "\n",
        "- Curva ROC y su respectiva métrica AUC-ROC.\n",
        "\n",
        "- Curva Precision–Recall y PR-AUC (Average Precision), especialmente relevante en contextos donde la clase de interés es minoritaria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fd2e529",
      "metadata": {
        "id": "9fd2e529"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 0) Librerías\n",
        "# =========================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5aa2d907",
      "metadata": {
        "id": "5aa2d907"
      },
      "source": [
        "##Primer paso\n",
        "\n",
        "Se utiliza el dataset data-churn.csv, el cual contiene información de clientes de una empresa de telecomunicaciones. A partir de este conjunto de datos, se definen las siguientes variables:\n",
        "\n",
        "Variable objetivo: Churn, de tipo binaria, donde Yes indica que el cliente abandona el servicio (1) y No que permanece (0).\n",
        "\n",
        "Variables numéricas: MonthlyCharges, TotalCharges y tenure, las cuales representan características cuantitativas del comportamiento y antigüedad del cliente.\n",
        "\n",
        "Variables categóricas: Contract, InternetService, PaymentMethod y PhoneService, que describen distintos atributos del tipo de servicio y condiciones contractuales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6837963c",
      "metadata": {
        "id": "6837963c"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1) Carga de datos\n",
        "# =========================\n",
        "df = pd.read_csv(\"data-churn.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edde80b6",
      "metadata": {
        "id": "edde80b6"
      },
      "outputs": [],
      "source": [
        "# Variable objetivo: Yes/No -> 1/0\n",
        "df[\"Churn\"] = df[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "print(\"Tamaño del dataset:\", df.shape)\n",
        "print(\"\\nProporción de clases (Churn):\")\n",
        "display(df[\"Churn\"].value_counts(normalize=True).rename(\"proporción\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32826e3",
      "metadata": {
        "id": "c32826e3"
      },
      "outputs": [],
      "source": [
        "# Estadísticos básicos (rápidos)\n",
        "display(df.describe(include=\"all\").transpose().head(25))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fd4950c",
      "metadata": {
        "id": "3fd4950c"
      },
      "source": [
        "## Segundo paso\n",
        "\n",
        "En esta etapa se definen las principales decisiones de preprocesamiento, orientadas a preparar los datos de forma adecuada para el entrenamiento de los modelos:\n",
        "\n",
        "Tratamiento de valores faltantes: se aplica una imputación simple, utilizando la mediana para las variables numéricas y la categoría más frecuente para las variables categóricas, con el fin de evitar la pérdida de información.\n",
        "\n",
        "Variables categóricas: se utiliza One-Hot Encoding con el parámetro drop='first', lo que permite representar las categorías en forma numérica y, al mismo tiempo, evitar problemas de multicolinealidad perfecta.\n",
        "\n",
        "Variables numéricas: se realiza una estandarización mediante StandardScaler, asegurando que todas las variables se encuentren en una escala comparable y facilitando la convergencia del modelo de Regresión Logística."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb1e2038",
      "metadata": {
        "id": "bb1e2038"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 2) Selección de variables\n",
        "# =========================\n",
        "numeric_features = [\"MonthlyCharges\", \"TotalCharges\", \"tenure\"]\n",
        "categorical_features = [\"Contract\", \"InternetService\", \"PaymentMethod\", \"PhoneService\"]\n",
        "\n",
        "X = df[numeric_features + categorical_features].copy()\n",
        "y = df[\"Churn\"].copy()\n",
        "\n",
        "# Asegurar tipo numérico en columnas numéricas (por ejemplo TotalCharges viene como texto)\n",
        "for col in numeric_features:\n",
        "    X[col] = pd.to_numeric(X[col], errors=\"coerce\")\n",
        "\n",
        "print(\"Nulos por columna en X:\")\n",
        "display(X.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db34144d",
      "metadata": {
        "id": "db34144d"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3) Preprocesador base\n",
        "# =========================\n",
        "numeric_transformer_base = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor_base = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer_base, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac8b948",
      "metadata": {
        "id": "0ac8b948"
      },
      "source": [
        "## Función de evaluación con k-fold estratificado\n",
        "\n",
        "Se define una función de evaluación que permite analizar el desempeño de los modelos de forma consistente a través de los distintos folds de la validación cruzada estratificada. Esta función:\n",
        "\n",
        "Calcula las principales métricas de clasificación en cada fold, incluyendo accuracy, precision, recall, F1, ROC-AUC y PR-AUC.\n",
        "\n",
        "Resume los resultados mediante una tabla que presenta el promedio y la desviación estándar de cada métrica, facilitando la comparación entre modelos.\n",
        "\n",
        "Genera las curvas ROC promedio y Precision–Recall promedio, obtenidas a partir del promedio de las curvas de cada fold mediante interpolación, lo que permite una evaluación visual más estable del desempeño global del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5240e98f",
      "metadata": {
        "id": "5240e98f"
      },
      "outputs": [],
      "source": [
        "def evaluate_model_cv(model, X, y, cv_splits=5, title=\"Modelo\"):\n",
        "    skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    fold_rows = []\n",
        "    roc_curves = []\n",
        "    pr_curves = []\n",
        "\n",
        "    # Grillas comunes para promediar curvas\n",
        "    mean_fpr = np.linspace(0, 1, 200)\n",
        "    mean_recall = np.linspace(0, 1, 200)\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # Métricas\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        p, r, _ = precision_recall_curve(y_test, y_proba)\n",
        "        pr_auc = average_precision_score(y_test, y_proba)\n",
        "\n",
        "        fold_rows.append({\n",
        "            \"fold\": fold,\n",
        "            \"accuracy\": acc,\n",
        "            \"precision\": prec,\n",
        "            \"recall\": rec,\n",
        "            \"f1\": f1,\n",
        "            \"roc_auc\": roc_auc,\n",
        "            \"pr_auc\": pr_auc\n",
        "        })\n",
        "\n",
        "        # Curvas interpoladas para promediar\n",
        "        tpr_interp = np.interp(mean_fpr, fpr, tpr)\n",
        "        tpr_interp[0] = 0.0\n",
        "        roc_curves.append(tpr_interp)\n",
        "\n",
        "        # PR: interpolar precision en función de recall (recall viene descendente a veces)\n",
        "        # Ordenamos por recall ascendente\n",
        "        order = np.argsort(r)\n",
        "        r_sorted = r[order]\n",
        "        p_sorted = p[order]\n",
        "        p_interp = np.interp(mean_recall, r_sorted, p_sorted)\n",
        "        pr_curves.append(p_interp)\n",
        "\n",
        "    results = pd.DataFrame(fold_rows)\n",
        "\n",
        "    # Resumen\n",
        "    summary = pd.DataFrame({\n",
        "        \"mean\": results.drop(columns=[\"fold\"]).mean(),\n",
        "        \"std\": results.drop(columns=[\"fold\"]).std()\n",
        "    })\n",
        "\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    display(results)\n",
        "    display(summary)\n",
        "\n",
        "    # Matriz de confusión global (juntando predicciones de folds de forma reproducible)\n",
        "    # (Para mantener simple: re-ejecutamos y acumulamos)\n",
        "    y_true_all, y_pred_all, y_proba_all = [], [], []\n",
        "    for train_idx, test_idx in skf.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_proba = model.predict_proba(X_test)[:, 1]\n",
        "        y_true_all.extend(y_test)\n",
        "        y_pred_all.extend(y_pred)\n",
        "        y_proba_all.extend(y_proba)\n",
        "\n",
        "    cm = confusion_matrix(y_true_all, y_pred_all)\n",
        "    print(\"Matriz de confusión global (todos los folds):\")\n",
        "    print(cm)\n",
        "    print(\"\\nReporte de clasificación global:\")\n",
        "    print(classification_report(y_true_all, y_pred_all, digits=3))\n",
        "\n",
        "    # Curvas promedio\n",
        "    mean_tpr = np.mean(roc_curves, axis=0)\n",
        "    mean_tpr[-1] = 1.0\n",
        "    mean_roc_auc = auc(mean_fpr, mean_tpr)\n",
        "\n",
        "    mean_precision = np.mean(pr_curves, axis=0)\n",
        "    mean_pr_auc = auc(mean_recall, mean_precision)  # área aproximada sobre curva promedio\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(mean_fpr, mean_tpr, label=f\"ROC promedio (AUC≈{mean_roc_auc:.3f})\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "    plt.title(f\"Curva ROC promedio – {title}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(mean_recall, mean_precision, label=f\"PR promedio (AUC≈{mean_pr_auc:.3f})\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(f\"Curva Precision–Recall promedio – {title}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return {\"per_fold\": results, \"summary\": summary, \"cm\": cm}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af224f65",
      "metadata": {
        "id": "af224f65"
      },
      "source": [
        "## Tercer paso\n",
        "\n",
        "Se entrena un modelo de Regresión Logística utilizando las variables previamente preprocesadas. Con el objetivo de aproximar un modelo sin regularización significativa, se emplea una penalización L2 con un valor de C suficientemente grande, lo que en la práctica equivale a una regularización muy débil y permite analizar el comportamiento del modelo base sin un control estricto de complejidad.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c063407b",
      "metadata": {
        "id": "c063407b"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 4) Modelo base\n",
        "# =========================\n",
        "log_reg_base = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_base),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        penalty=\"l2\",\n",
        "        C=1e6,           # C grande ~ regularización muy débil (casi 'none')\n",
        "        solver=\"lbfgs\",\n",
        "        max_iter=2000,\n",
        "        random_state=RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "\n",
        "results_base = evaluate_model_cv(log_reg_base, X, y, cv_splits=5, title=\"LogReg Base (sin polinomios)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6f874fd",
      "metadata": {
        "id": "b6f874fd"
      },
      "source": [
        "## Cuarto paso\n",
        "\n",
        "De acuerdo con la pauta de la actividad, se incorporan transformaciones polinomiales de grado 2, las cuales incluyen términos de interacción, con el objetivo de capturar relaciones más complejas entre las variables numéricas.\n",
        "\n",
        "Estas transformaciones se aplican sobre un subconjunto de variables consideradas relevantes. En este caso, y manteniendo la coherencia con el código base utilizado, se seleccionan las siguientes variables numéricas:\n",
        "\n",
        "- MonthlyCharges\n",
        "\n",
        "- TotalCharges\n",
        "\n",
        "- tenure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55ab22d",
      "metadata": {
        "id": "b55ab22d"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 5) Preprocesador con polinomios (grado 2)\n",
        "# =========================\n",
        "selected_num = [\"MonthlyCharges\", \"TotalCharges\", \"tenure\"]\n",
        "\n",
        "numeric_transformer_poly = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False))\n",
        "])\n",
        "\n",
        "preprocessor_poly = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"poly\", numeric_transformer_poly, selected_num),\n",
        "        (\"cat\", categorical_transformer, categorical_features),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dad4ce5d",
      "metadata": {
        "id": "dad4ce5d"
      },
      "outputs": [],
      "source": [
        "# Modelo con polinomios (sin regularización fuerte)\n",
        "log_reg_poly = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_poly),\n",
        "    (\"clf\", LogisticRegression(\n",
        "        penalty=\"l2\",\n",
        "        C=1e6,\n",
        "        solver=\"lbfgs\",\n",
        "        max_iter=3000,\n",
        "        random_state=RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "\n",
        "results_poly = evaluate_model_cv(log_reg_poly, X, y, cv_splits=5, title=\"LogReg + Polinomios (grado 2)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab610a8a",
      "metadata": {
        "id": "ab610a8a"
      },
      "source": [
        "## Paso 4. Regularización (penalizaciones)\n",
        "\n",
        "Se evalúan al menos dos variantes de regularización con el fin de controlar la complejidad del modelo:\n",
        "\n",
        "L2 (Ridge): penaliza la magnitud de los coeficientes, lo que generalmente produce modelos más estables y reduce el riesgo de sobreajuste.\n",
        "\n",
        "L1 (Lasso) o Elastic Net: introduce esparsidad en los coeficientes, permitiendo que algunas variables tengan una contribución nula y facilitando, de este modo, la selección automática de variables.\n",
        "\n",
        "Para la selección de los hiperparámetros se utiliza como métrica principal PR-AUC (Average Precision), ya que resulta especialmente adecuada en problemas de clasificación con desbalance de clases, como el caso del churn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ee0be08",
      "metadata": {
        "id": "3ee0be08"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 6) GridSearch – L2 (Ridge)\n",
        "# =========================\n",
        "param_grid_l2 = {\n",
        "    \"clf__penalty\": [\"l2\"],\n",
        "    \"clf__solver\": [\"lbfgs\"],\n",
        "    \"clf__C\": [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "}\n",
        "\n",
        "base_pipeline_poly = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_poly),\n",
        "    (\"clf\", LogisticRegression(max_iter=4000, random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "grid_l2 = GridSearchCV(\n",
        "    base_pipeline_poly,\n",
        "    param_grid_l2,\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "    scoring=\"average_precision\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_l2.fit(X, y)\n",
        "print(\"Mejores hiperparámetros (L2):\", grid_l2.best_params_)\n",
        "best_model_l2 = grid_l2.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73351a45",
      "metadata": {
        "id": "73351a45"
      },
      "outputs": [],
      "source": [
        "results_l2 = evaluate_model_cv(best_model_l2, X, y, cv_splits=5, title=\"LogReg + Polinomios + L2 (tuneado)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2510aa04",
      "metadata": {
        "id": "2510aa04"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 7) GridSearch – Elastic Net (incluye L1 como caso límite)\n",
        "# =========================\n",
        "param_grid_en = {\n",
        "    \"clf__penalty\": [\"elasticnet\"],\n",
        "    \"clf__solver\": [\"saga\"],\n",
        "    \"clf__l1_ratio\": [0.2, 0.5, 0.8],\n",
        "    \"clf__C\": [0.01, 0.1, 1.0, 10.0]\n",
        "}\n",
        "\n",
        "pipeline_en = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor_poly),\n",
        "    (\"clf\", LogisticRegression(max_iter=6000, random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "grid_en = GridSearchCV(\n",
        "    pipeline_en,\n",
        "    param_grid_en,\n",
        "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
        "    scoring=\"average_precision\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_en.fit(X, y)\n",
        "print(\"Mejores hiperparámetros (Elastic Net):\", grid_en.best_params_)\n",
        "best_model_en = grid_en.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0319d321",
      "metadata": {
        "id": "0319d321"
      },
      "outputs": [],
      "source": [
        "results_en = evaluate_model_cv(best_model_en, X, y, cv_splits=5, title=\"LogReg + Polinomios + Elastic Net (tuneado)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b1f971",
      "metadata": {
        "id": "33b1f971"
      },
      "source": [
        "## Comparación final de modelos\n",
        "\n",
        "Se construye una tabla resumen que presenta, para cada alternativa evaluada, las métricas de desempeño expresadas como promedio y desviación estándar, lo que permite comparar de manera clara y consistente el rendimiento de los distintos modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e5213a1",
      "metadata": {
        "id": "3e5213a1"
      },
      "outputs": [],
      "source": [
        "def extract_summary(res_dict, model_name):\n",
        "    s = res_dict[\"summary\"][\"mean\"].copy()\n",
        "    s_std = res_dict[\"summary\"][\"std\"].copy()\n",
        "    out = pd.DataFrame({\n",
        "        \"metric_mean\": s,\n",
        "        \"metric_std\": s_std\n",
        "    })\n",
        "    out[\"model\"] = model_name\n",
        "    return out.reset_index().rename(columns={\"index\": \"metric\"})\n",
        "\n",
        "summary_all = pd.concat([\n",
        "    extract_summary(results_base, \"Base\"),\n",
        "    extract_summary(results_poly, \"Polinomios g2\"),\n",
        "    extract_summary(results_l2, \"Polinomios g2 + L2\"),\n",
        "    extract_summary(results_en, \"Polinomios g2 + ElasticNet\"),\n",
        "], ignore_index=True)\n",
        "\n",
        "pivot = summary_all.pivot(index=\"metric\", columns=\"model\", values=\"metric_mean\")\n",
        "pivot_std = summary_all.pivot(index=\"metric\", columns=\"model\", values=\"metric_std\")\n",
        "\n",
        "print(\"Promedios por métrica:\")\n",
        "display(pivot)\n",
        "\n",
        "print(\"Desviación estándar por métrica:\")\n",
        "display(pivot_std)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2620b11",
      "metadata": {
        "id": "a2620b11"
      },
      "source": [
        "**¿Qué tan desbalanceado está el churn y por qué la accuracy puede ser engañosa?**\n",
        "\n",
        "En el problema de churn, la cantidad de clientes que efectivamente abandonan el servicio suele ser menor que la de quienes permanecen. Esto genera un desbalance de clases. En este contexto, un modelo que predice mayoritariamente que los clientes no se irán (churn = 0) puede alcanzar una accuracy alta, aun cuando falle en identificar a muchos de los clientes que sí se van.\n",
        "Por esta razón, la accuracy por sí sola no es suficiente. Es fundamental analizar la matriz de confusión y métricas como recall y F1 para la clase churn, así como PR-AUC, que es más informativa cuando las clases están desbalanceadas.\n",
        "\n",
        "**¿Qué modelo seleccionar y por qué?**\n",
        "\n",
        "En problemas de churn, el modelo más adecuado no es necesariamente el que maximiza la accuracy, sino aquel que logra un buen equilibrio entre identificar correctamente a los clientes que se irán y mantener un nivel razonable de errores.\n",
        "En este trabajo se comparan tres enfoques: un modelo base, uno con transformaciones polinomiales y modelos regularizados. La selección final se basa principalmente en métricas como PR-AUC y F1, además de la estabilidad de los resultados a lo largo de los distintos folds de la validación cruzada.\n",
        "\n",
        "**Efecto de las transformaciones polinomiales en el rendimiento y la complejidad**\n",
        "\n",
        "Las transformaciones polinomiales permiten capturar relaciones más complejas entre las variables, lo que puede mejorar el desempeño del modelo. Sin embargo, también aumentan el número de variables y la complejidad del modelo, lo que incrementa el riesgo de sobreajuste si no se controla adecuadamente.\n",
        "\n",
        "**Efecto de la regularización (L2 y Elastic Net)**\n",
        "\n",
        "La regularización ayuda a controlar la complejidad del modelo. La penalización L2 tiende a reducir la magnitud de los coeficientes, lo que suele traducirse en modelos más estables y con mejor capacidad de generalización.\n",
        "Por su parte, Elastic Net combina penalización L1 y L2, lo que permite reducir o incluso eliminar variables menos relevantes. Su efecto depende del valor del parámetro l1_ratio, que define el equilibrio entre ambas penalizaciones.\n",
        "\n",
        "**Trade-off entre recall y precision desde el punto de vista del negocio**\n",
        "\n",
        "Maximizar el recall implica reducir la cantidad de clientes que se van sin ser detectados por el modelo, lo cual es deseable desde el punto de vista del negocio. Sin embargo, esto puede aumentar los falsos positivos, es decir, clientes que el modelo identifica como posibles fugas pero que en realidad no se irían.\n",
        "La curva Precision–Recall permite analizar este compromiso y ajustar el umbral de decisión de acuerdo con la capacidad operativa del equipo de retención y los costos asociados a contactar clientes innecesariamente.\n",
        "\n",
        "**Otras técnicas que podrían mejorar el rendimiento**\n",
        "\n",
        "Además de los enfoques evaluados, existen otras estrategias que podrían explorarse para mejorar el desempeño del modelo, como ajustar el umbral de decisión en lugar de utilizar el valor fijo de 0.5, incorporar pesos de clase para manejar el desbalance, aplicar técnicas de re-muestreo como sobremuestreo o submuestreo, o probar modelos más complejos y calibrar sus probabilidades."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12bd9be4",
      "metadata": {
        "id": "12bd9be4"
      },
      "source": [
        "## Conclusiones\n",
        "\n",
        "Se desarrolló un flujo completo de preprocesamiento y modelamiento orientado a la predicción de churn mediante Regresión Logística. A lo largo del análisis, se comparó un modelo base con versiones que incorporan términos polinomiales y distintos esquemas de regularización, específicamente L2 y Elastic Net.\n",
        "\n",
        "Dado el desbalance presente en la variable objetivo, se priorizó el uso de métricas como F1 y PR-AUC, las cuales entregan una evaluación más representativa del desempeño del modelo que la accuracy por sí sola. Finalmente, se observó que la regularización cumple un rol clave al controlar la complejidad introducida por las transformaciones polinomiales, contribuyendo además a una mayor estabilidad de los resultados a través de los distintos folds de la validación cruzada."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}