{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#ACTIVIDAD 3 ML II\n",
        "\n",
        "En esta actividad se aborda el problema de predicción de fuga de clientes (churn) utilizando enfoques de clasificación distintos a los vistos anteriormente. Mientras que en actividades previas se trabajó con regresión logística y modelos basados en árboles, en esta oportunidad el foco está en Naïve Bayes y Support Vector Machines (SVM), dos familias de modelos con fundamentos conceptuales diferentes.\n",
        "\n",
        "El objetivo principal es analizar cómo estos modelos se comportan frente a un problema real de clasificación binaria con desbalance de clases, evaluando no solo su desempeño predictivo, sino también aspectos como el costo computacional, la estabilidad de los resultados y la facilidad de interpretación. Para asegurar una comparación justa, se utiliza el mismo dataset y el mismo flujo de preprocesamiento definidos en las actividades anteriores.\n",
        "\n",
        "Naïve Bayes se presenta como un modelo probabilístico simple y eficiente, basado en el supuesto de independencia condicional entre las variables explicativas. A pesar de que este supuesto rara vez se cumple estrictamente en datos reales, el modelo suele ofrecer resultados competitivos como línea base, especialmente por su rapidez de entrenamiento y evaluación.\n",
        "\n",
        "Por otro lado, Support Vector Machines permiten construir fronteras de decisión más flexibles. En esta actividad se analizan dos variantes: una versión lineal, que busca una separación simple entre clases, y una versión con kernel RBF, capaz de capturar relaciones no lineales más complejas. En ambos casos, se estudia el impacto del escalamiento de variables y de la selección de hiperparámetros en el desempeño del modelo.\n",
        "\n",
        "La evaluación se realiza utilizando métricas adecuadas para problemas desbalanceados, como F1 y PR-AUC, además de curvas ROC y Precision–Recall. Finalmente, se discuten los resultados desde una perspectiva tanto técnica como de negocio, considerando el contexto de campañas de retención de clientes, donde identificar correctamente a quienes tienen mayor probabilidad de abandono resulta especialmente relevante."
      ],
      "metadata": {
        "id": "gSoBWLWSVi9X"
      },
      "id": "gSoBWLWSVi9X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "030fc014",
      "metadata": {
        "id": "030fc014"
      },
      "outputs": [],
      "source": [
        "# Librerías base\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# Visualización\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, Binarizer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Modelos\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "# Métricas y curvas\n",
        "from sklearn.metrics import (\n",
        "    roc_curve, precision_recall_curve\n",
        ")\n",
        "\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db8531ce",
      "metadata": {
        "id": "db8531ce"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"data-churn.csv\")\n",
        "\n",
        "df[\"Churn\"] = df[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "target = \"Churn\"\n",
        "\n",
        "X = df.drop(columns=[target]).copy()\n",
        "y = df[target].copy()\n",
        "\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Target distribution:\\n\", y.value_counts(normalize=True))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "651be608",
      "metadata": {
        "id": "651be608"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80ab3ef4",
      "metadata": {
        "id": "80ab3ef4"
      },
      "outputs": [],
      "source": [
        "ohe_dense = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "\n",
        "numeric_transformer_nb = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer_nb = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", ohe_dense)\n",
        "])\n",
        "\n",
        "preprocessor_nb = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer_nb, numeric_features),\n",
        "        (\"cat\", categorical_transformer_nb, categorical_features),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "pipe_nb = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor_nb),\n",
        "    (\"clf\", GaussianNB())\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "222de71c",
      "metadata": {
        "id": "222de71c"
      },
      "outputs": [],
      "source": [
        "param_grid_nb = [\n",
        "    {\n",
        "        \"clf\": [GaussianNB()],\n",
        "        \"clf__var_smoothing\": np.logspace(-12, -7, 6)\n",
        "    },\n",
        "    {\n",
        "        \"clf\": [BernoulliNB()],\n",
        "        \"clf__alpha\": [0.1, 0.5, 1.0, 2.0, 5.0],\n",
        "        \"clf__binarize\": [0.0, 0.5, 1.0]\n",
        "    }\n",
        "]\n",
        "\n",
        "scoring_main = \"f1\"  # o \"average_precision\"\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "grid_nb = GridSearchCV(\n",
        "    estimator=pipe_nb,\n",
        "    param_grid=param_grid_nb,\n",
        "    scoring=scoring_main,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "grid_nb.fit(X_train, y_train)\n",
        "t_nb = time.perf_counter() - t0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f650c56",
      "metadata": {
        "id": "4f650c56"
      },
      "outputs": [],
      "source": [
        "print(\"=== Naïve Bayes ===\")\n",
        "print(\"Best params:\", grid_nb.best_params_)\n",
        "print(f\"CV best {scoring_main}: {grid_nb.best_score_:.4f}\")\n",
        "print(f\"GridSearch time (s): {t_nb:.2f}\")\n",
        "\n",
        "best_nb = grid_nb.best_estimator_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad2b3260",
      "metadata": {
        "id": "ad2b3260"
      },
      "outputs": [],
      "source": [
        "nb_metrics, nb_pred, nb_score = report_test_metrics(best_nb, X_test, y_test, label=\"NaiveBayes\")\n",
        "print(pd.Series(nb_metrics))\n",
        "\n",
        "plot_confusion(y_test, nb_pred, title=\"Naïve Bayes — Confusion Matrix (test)\")\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_test, nb_score)\n",
        "prec, rec, _ = precision_recall_curve(y_test, nb_score)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(fpr, tpr)\n",
        "ax.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "ax.set_title(\"Naïve Bayes — ROC (test)\")\n",
        "ax.set_xlabel(\"False Positive Rate\")\n",
        "ax.set_ylabel(\"True Positive Rate\")\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(rec, prec)\n",
        "ax.set_title(\"Naïve Bayes — Precision–Recall (test)\")\n",
        "ax.set_xlabel(\"Recall\")\n",
        "ax.set_ylabel(\"Precision\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583598f7",
      "metadata": {
        "id": "583598f7"
      },
      "outputs": [],
      "source": [
        "ohe_sparse = OneHotEncoder(handle_unknown=\"ignore\")\n",
        "\n",
        "numeric_transformer_svm = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer_svm = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", ohe_sparse)\n",
        "])\n",
        "\n",
        "preprocessor_svm = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer_svm, numeric_features),\n",
        "        (\"cat\", categorical_transformer_svm, categorical_features),\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc570f0",
      "metadata": {
        "id": "0fc570f0"
      },
      "outputs": [],
      "source": [
        "pipe_linsvm = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor_svm),\n",
        "    (\"clf\", CalibratedClassifierCV(\n",
        "        estimator=LinearSVC(dual=True, max_iter=20000),\n",
        "        method=\"sigmoid\",\n",
        "        cv=3\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_grid_linsvm = {\n",
        "    \"clf__estimator__C\": [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "}\n",
        "\n",
        "pipe_rbf = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor_svm),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", probability=False))\n",
        "])\n",
        "\n",
        "param_grid_rbf = {\n",
        "    \"clf__C\": [0.1, 1.0, 10.0, 100.0],\n",
        "    \"clf__gamma\": [\"scale\", \"auto\", 1e-3, 1e-2, 1e-1]\n",
        "}\n",
        "\n",
        "scoring_main = \"f1\"  # o \"average_precision\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c45f194",
      "metadata": {
        "id": "7c45f194"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n=== SVM Lineal (Grid) ===\")\n",
        "grid_linsvm = GridSearchCV(pipe_linsvm, param_grid_linsvm, scoring=scoring_main, cv=cv, n_jobs=-1)\n",
        "grid_linsvm, t_grid_lin = timed_search(grid_linsvm, X_train, y_train, label=\"Grid LinearSVM\")\n",
        "\n",
        "print(\"\\n=== SVM RBF (Grid) ===\")\n",
        "grid_rbf = GridSearchCV(pipe_rbf, param_grid_rbf, scoring=scoring_main, cv=cv, n_jobs=-1)\n",
        "grid_rbf, t_grid_rbf = timed_search(grid_rbf, X_train, y_train, label=\"Grid RBF-SVM\")\n",
        "\n",
        "print(\"\\n=== SVM Lineal (Random) ===\")\n",
        "rand_linsvm = RandomizedSearchCV(\n",
        "    pipe_linsvm,\n",
        "    param_distributions={\"clf__estimator__C\": loguniform(1e-3, 1e2)},\n",
        "    n_iter=20,\n",
        "    scoring=scoring_main,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "rand_linsvm, t_rand_lin = timed_search(rand_linsvm, X_train, y_train, label=\"Random LinearSVM\")\n",
        "\n",
        "print(\"\\n=== SVM RBF (Random) ===\")\n",
        "rand_rbf = RandomizedSearchCV(\n",
        "    pipe_rbf,\n",
        "    param_distributions={\"clf__C\": loguniform(1e-2, 1e3), \"clf__gamma\": loguniform(1e-4, 1e0)},\n",
        "    n_iter=30,\n",
        "    scoring=scoring_main,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "rand_rbf, t_rand_rbf = timed_search(rand_rbf, X_train, y_train, label=\"Random RBF-SVM\")\n",
        "\n",
        "# Elegimos el mejor (según score CV) para cada familia\n",
        "best_lin = grid_linsvm.best_estimator_ if grid_linsvm.best_score_ >= rand_linsvm.best_score_ else rand_linsvm.best_estimator_\n",
        "best_rbf = grid_rbf.best_estimator_ if grid_rbf.best_score_ >= rand_rbf.best_score_ else rand_rbf.best_estimator_\n",
        "\n",
        "# ---- Reporte final en test (métricas, curvas) :contentReference[oaicite:12]{index=12}\n",
        "lin_metrics, lin_pred, lin_score = report_test_metrics(best_lin, X_test, y_test, label=\"SVM Linear (best)\")\n",
        "rbf_metrics, rbf_pred, rbf_score = report_test_metrics(best_rbf, X_test, y_test, label=\"SVM RBF (best)\")\n",
        "\n",
        "results = pd.DataFrame([nb_metrics, lin_metrics, rbf_metrics]).set_index(\"model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "081ccb33",
      "metadata": {
        "id": "081ccb33"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== Test metrics summary ===\")\n",
        "display(results)\n",
        "\n",
        "plot_confusion(y_test, lin_pred, title=\"SVM Linear — Confusion Matrix (test)\")\n",
        "plot_confusion(y_test, rbf_pred, title=\"SVM RBF — Confusion Matrix (test)\")\n",
        "\n",
        "# Curvas ROC/PR en test\n",
        "\n",
        "\n",
        "plot_roc_pr(y_test, lin_score, title=\"SVM Linear (best)\")\n",
        "plot_roc_pr(y_test, rbf_score, title=\"SVM RBF (best)\")\n",
        "\n",
        "# ============================================================\n",
        "# 2e) Desbalance: repetir mejor SVM con class_weight='balanced' :contentReference[oaicite:13]{index=13}\n",
        "# ============================================================\n",
        "\n",
        "# Reentrena SOLO la mejor familia (ejemplo: RBF). Puedes hacerlo para ambos si quieres.\n",
        "best_rbf_balanced = Pipeline(steps=[\n",
        "    (\"preprocessor\", preprocessor_svm),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", class_weight=\"balanced\"))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f9d603f",
      "metadata": {
        "id": "2f9d603f"
      },
      "outputs": [],
      "source": [
        "# Usamos los mejores hiperparámetros encontrados\n",
        "best_params = {}\n",
        "if hasattr(best_rbf, \"get_params\"):\n",
        "    bp = best_rbf.get_params()\n",
        "    # Extraemos C y gamma si venían\n",
        "    if \"clf__C\" in bp: best_params[\"clf__C\"] = bp[\"clf__C\"]\n",
        "    if \"clf__gamma\" in bp: best_params[\"clf__gamma\"] = bp[\"clf__gamma\"]\n",
        "\n",
        "best_rbf_balanced.set_params(**best_params)\n",
        "best_rbf_balanced.fit(X_train, y_train)\n",
        "\n",
        "rbf_bal_metrics, rbf_bal_pred, rbf_bal_score = report_test_metrics(\n",
        "    best_rbf_balanced, X_test, y_test, label=\"SVM RBF balanced\"\n",
        ")\n",
        "\n",
        "print(\"\\n=== SVM RBF balanced (test) ===\")\n",
        "print(pd.Series(rbf_bal_metrics))\n",
        "plot_confusion(y_test, rbf_bal_pred, title=\"SVM RBF balanced — Confusion Matrix (test)\")\n",
        "plot_roc_pr(y_test, rbf_bal_score, title=\"SVM RBF balanced\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "817beace",
      "metadata": {
        "id": "817beace"
      },
      "source": [
        "# Actividad 3 – Machine Learning II  \n",
        "## Naïve Bayes y SVM para predicción de Churn\n",
        "\n",
        "Este notebook se basa en el código base entregado. Las celdas originales se mantienen y se agregan secciones nuevas para completar los requerimientos de la Actividad 3.\n",
        "\n",
        "A lo largo del desarrollo se utiliza el mismo dataset y el mismo preprocesamiento base definido en actividades previas (imputación, one-hot encoding y escalamiento de variables numéricas), con el objetivo de mantener comparabilidad entre modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3565d00d",
      "metadata": {
        "id": "3565d00d"
      },
      "outputs": [],
      "source": [
        "# ==========================================================\n",
        "# Ajuste de compatibilidad: si no existe utils.py en el entorno\n",
        "# ==========================================================\n",
        "# El código base importa \"from utils import *\". Si ese archivo no está disponible\n",
        "# en tu entorno (por ejemplo, al ejecutar en otra máquina), definimos aquí las\n",
        "# funciones mínimas necesarias para continuar sin errores.\n",
        "#\n",
        "# Nota: si utils.py sí existe, esta celda no cambia nada relevante.\n",
        "try:\n",
        "    _ = evaluate_model_cv_detailed  # noqa: F821\n",
        "    _ = get_feature_names_from_preprocessor  # noqa: F821\n",
        "    print(\"✅ utils disponibles: se usará la implementación original.\")\n",
        "except Exception:\n",
        "    print(\"⚠️ utils no detectado: se crearán funciones auxiliares mínimas.\")\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "    from sklearn.metrics import (\n",
        "        confusion_matrix, classification_report,\n",
        "        accuracy_score, precision_score, recall_score, f1_score,\n",
        "        roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "    )\n",
        "\n",
        "    def get_feature_names_from_preprocessor(preprocessor):\n",
        "        feature_names = []\n",
        "        for name, transformer, cols in preprocessor.transformers_:\n",
        "            if name == \"remainder\" and transformer == \"drop\":\n",
        "                continue\n",
        "            if hasattr(transformer, \"named_steps\"):\n",
        "                last = list(transformer.named_steps.values())[-1]\n",
        "                if hasattr(last, \"get_feature_names_out\"):\n",
        "                    try:\n",
        "                        names = last.get_feature_names_out(cols)\n",
        "                    except Exception:\n",
        "                        names = last.get_feature_names_out()\n",
        "                    feature_names.extend(names)\n",
        "                else:\n",
        "                    feature_names.extend(cols)\n",
        "            else:\n",
        "                if hasattr(transformer, \"get_feature_names_out\"):\n",
        "                    try:\n",
        "                        names = transformer.get_feature_names_out(cols)\n",
        "                    except Exception:\n",
        "                        names = transformer.get_feature_names_out()\n",
        "                    feature_names.extend(names)\n",
        "                else:\n",
        "                    feature_names.extend(cols)\n",
        "        return [str(n) for n in feature_names]\n",
        "\n",
        "    def evaluate_model_cv_detailed(model, X, y, cv, plot_curves=True, verbose=True):\n",
        "        per_fold = []\n",
        "        roc_curves, pr_curves = [], []\n",
        "        mean_fpr = np.linspace(0, 1, 200)\n",
        "        mean_recall = np.linspace(0, 1, 200)\n",
        "\n",
        "        y_true_all, y_pred_all, y_proba_all = [], [], []\n",
        "\n",
        "        for fold, (tr, te) in enumerate(cv.split(X, y), start=1):\n",
        "            X_tr, X_te = X.iloc[tr], X.iloc[te]\n",
        "            y_tr, y_te = y.iloc[tr], y.iloc[te]\n",
        "\n",
        "            model.fit(X_tr, y_tr)\n",
        "            # Probabilidades: si no existe predict_proba, usamos decision_function reescalada\n",
        "            if hasattr(model, \"predict_proba\"):\n",
        "                y_proba = model.predict_proba(X_te)[:, 1]\n",
        "            else:\n",
        "                scores = model.decision_function(X_te)\n",
        "                # Reescalado simple a (0,1) para poder graficar y calcular PR-AUC\n",
        "                y_proba = (scores - scores.min()) / (scores.max() - scores.min() + 1e-9)\n",
        "\n",
        "            y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "            acc = accuracy_score(y_te, y_pred)\n",
        "            prec = precision_score(y_te, y_pred, zero_division=0)\n",
        "            rec = recall_score(y_te, y_pred, zero_division=0)\n",
        "            f1 = f1_score(y_te, y_pred, zero_division=0)\n",
        "            pr_auc = average_precision_score(y_te, y_proba)\n",
        "\n",
        "            fpr, tpr, _ = roc_curve(y_te, y_proba)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            per_fold.append({\n",
        "                \"fold\": fold, \"accuracy\": acc, \"precision\": prec, \"recall\": rec,\n",
        "                \"f1\": f1, \"roc_auc\": roc_auc, \"pr_auc\": pr_auc\n",
        "            })\n",
        "\n",
        "            tpr_i = np.interp(mean_fpr, fpr, tpr)\n",
        "            tpr_i[0] = 0.0\n",
        "            roc_curves.append(tpr_i)\n",
        "\n",
        "            p, r, _ = precision_recall_curve(y_te, y_proba)\n",
        "            order = np.argsort(r)\n",
        "            r_s, p_s = r[order], p[order]\n",
        "            p_i = np.interp(mean_recall, r_s, p_s)\n",
        "            pr_curves.append(p_i)\n",
        "\n",
        "            y_true_all.extend(y_te)\n",
        "            y_pred_all.extend(y_pred)\n",
        "            y_proba_all.extend(y_proba)\n",
        "\n",
        "        df_fold = pd.DataFrame(per_fold)\n",
        "        summary = pd.DataFrame({\n",
        "            \"mean\": df_fold.drop(columns=[\"fold\"]).mean(),\n",
        "            \"std\": df_fold.drop(columns=[\"fold\"]).std()\n",
        "        })\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Resultados por fold:\")\n",
        "            display(df_fold)\n",
        "            print(\"\\nResumen (promedio y desviación estándar):\")\n",
        "            display(summary)\n",
        "\n",
        "            cm = confusion_matrix(y_true_all, y_pred_all)\n",
        "            print(\"\\nMatriz de confusión (global):\\n\", cm)\n",
        "            print(\"\\nReporte de clasificación (global):\\n\", classification_report(y_true_all, y_pred_all, digits=3))\n",
        "\n",
        "        curves = None\n",
        "        if plot_curves:\n",
        "            mean_tpr = np.mean(roc_curves, axis=0)\n",
        "            mean_tpr[-1] = 1.0\n",
        "            mean_roc_auc = auc(mean_fpr, mean_tpr)\n",
        "\n",
        "            mean_precision = np.mean(pr_curves, axis=0)\n",
        "            mean_pr_auc = auc(mean_recall, mean_precision)\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(mean_fpr, mean_tpr, label=f\"ROC promedio (AUC≈{mean_roc_auc:.3f})\")\n",
        "            plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "            plt.xlabel(\"False Positive Rate\")\n",
        "            plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "            plt.title(\"Curva ROC promedio\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(mean_recall, mean_precision, label=f\"PR promedio (AUC≈{mean_pr_auc:.3f})\")\n",
        "            plt.xlabel(\"Recall\")\n",
        "            plt.ylabel(\"Precision\")\n",
        "            plt.title(\"Curva Precision–Recall promedio\")\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "\n",
        "            curves = {\"mean_fpr\": mean_fpr, \"mean_tpr\": mean_tpr,\n",
        "                      \"mean_recall\": mean_recall, \"mean_precision\": mean_precision}\n",
        "\n",
        "        return df_fold, summary, curves\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3768323",
      "metadata": {
        "id": "a3768323"
      },
      "source": [
        "## Primer Paso. Naïve Bayes\n",
        "\n",
        "En esta sección se implementa un clasificador **Naïve Bayes**. La idea principal es estimar la probabilidad de churn a partir de una combinación “simple” de evidencias (las variables), asumiendo que, dado el estado de churn, las variables se comportan como si fueran independientes entre sí.\n",
        "\n",
        "En la práctica, esta independencia no siempre se cumple. Aun así, Naïve Bayes suele funcionar bien como modelo base rápido, y su costo computacional es bajo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e91c8af",
      "metadata": {
        "id": "7e91c8af"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1.1 Variante de Naïve Bayes\n",
        "# =========================\n",
        "# Dado que el preprocesamiento incluye variables continuas escaladas y variables one-hot,\n",
        "# trabajaremos con GaussianNB, que modela features continuas con distribuciones normales.\n",
        "# Como GaussianNB espera una matriz densa, convertimos la salida del preprocesador a densa.\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Convertir a denso (si ya es denso, no cambia)\n",
        "to_dense = FunctionTransformer(lambda x: x.toarray() if hasattr(x, \"toarray\") else x, accept_sparse=True)\n",
        "\n",
        "pipe_nb = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"to_dense\", to_dense),\n",
        "    (\"clf\", GaussianNB())\n",
        "])\n",
        "\n",
        "# Hiperparámetro principal en GaussianNB: var_smoothing\n",
        "# Probamos valores en escala logarítmica para cubrir rangos típicos sin exagerar el costo.\n",
        "param_grid_nb = {\n",
        "    \"clf__var_smoothing\": np.logspace(-12, -7, 6)\n",
        "}\n",
        "\n",
        "# Separamos un conjunto de test para reportar resultados finales (como pide la pauta)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "grid_nb = GridSearchCV(\n",
        "    estimator=pipe_nb,\n",
        "    param_grid=param_grid_nb,\n",
        "    cv=cv5,\n",
        "    scoring=\"average_precision\",   # PR-AUC como métrica principal (clase churn suele ser minoritaria)\n",
        "    n_jobs=-1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "grid_nb.fit(X_train, y_train)\n",
        "nb_search_time = time.perf_counter() - t0\n",
        "\n",
        "print(\"Tiempo GridSearch (Naïve Bayes):\", round(nb_search_time, 3), \"s\")\n",
        "print(\"Mejores hiperparámetros (NB):\")\n",
        "pprint(grid_nb.best_params_)\n",
        "print(\"Mejor score CV (PR-AUC):\", grid_nb.best_score_)\n",
        "\n",
        "best_nb = grid_nb.best_estimator_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f4f3b42",
      "metadata": {
        "id": "3f4f3b42"
      },
      "source": [
        "### 1.2 Justificación (en palabras simples)\n",
        "\n",
        "Se utiliza GaussianNB porque el conjunto final incluye variables numéricas continuas (escaladas) y variables categóricas codificadas. Este modelo asume que, dentro de cada clase (churn/no churn), las variables se distribuyen de forma aproximadamente “normal”. Aunque esta suposición es una simplificación, suele entregar resultados razonables como línea base y permite entrenar y evaluar muy rápido.\n",
        "\n",
        "Además, se mantiene el mismo preprocesamiento que en actividades anteriores para que la comparación sea justa.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20134f7a",
      "metadata": {
        "id": "20134f7a"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1.3 Evaluación final en TEST (métricas + curvas)\n",
        "# =========================\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix,\n",
        "    roc_curve, precision_recall_curve, auc\n",
        ")\n",
        "\n",
        "# Predicciones en test\n",
        "best_nb.fit(X_train, y_train)\n",
        "y_proba_nb = best_nb.predict_proba(X_test)[:, 1]\n",
        "y_pred_nb = (y_proba_nb >= 0.5).astype(int)\n",
        "\n",
        "print(\"Matriz de confusión (TEST):\")\n",
        "print(confusion_matrix(y_test, y_pred_nb))\n",
        "\n",
        "print(\"\\nMétricas (TEST):\")\n",
        "print(\"Accuracy :\", round(accuracy_score(y_test, y_pred_nb), 3))\n",
        "print(\"Precision:\", round(precision_score(y_test, y_pred_nb, zero_division=0), 3))\n",
        "print(\"Recall   :\", round(recall_score(y_test, y_pred_nb, zero_division=0), 3))\n",
        "print(\"F1       :\", round(f1_score(y_test, y_pred_nb, zero_division=0), 3))\n",
        "print(\"AUC-ROC  :\", round(roc_auc_score(y_test, y_proba_nb), 3))\n",
        "print(\"PR-AUC   :\", round(average_precision_score(y_test, y_proba_nb), 3))\n",
        "\n",
        "# Curvas en test\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba_nb)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "prec, rec, _ = precision_recall_curve(y_test, y_proba_nb)\n",
        "pr_auc = average_precision_score(y_test, y_proba_nb)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f\"ROC (AUC={roc_auc:.3f})\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "plt.title(\"Naïve Bayes – Curva ROC (TEST)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(rec, prec, label=f\"PR (AP={pr_auc:.3f})\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Naïve Bayes – Curva Precision–Recall (TEST)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46d1e0db",
      "metadata": {
        "id": "46d1e0db"
      },
      "source": [
        "### 1.4 Evaluación con k-fold (curvas promedio)\n",
        "\n",
        "Además del test final, se evalúa el modelo usando k-fold estratificado. Esto permite ver si el desempeño es consistente y no depende demasiado de una sola partición de datos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3fb069c",
      "metadata": {
        "id": "f3fb069c"
      },
      "outputs": [],
      "source": [
        "df_nb_cv, summary_nb_cv, curves_nb_cv = evaluate_model_cv_detailed(\n",
        "    best_nb, X, y, cv=cv5, plot_curves=True, verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd708a78",
      "metadata": {
        "id": "bd708a78"
      },
      "source": [
        "### 1.5 Dependencia entre predictores y su impacto en Naïve Bayes\n",
        "\n",
        "Naïve Bayes supone independencia condicional entre variables. Para tener una idea de qué tan razonable es esa suposición, cuantificamos dependencia entre algunas variables numéricas mediante correlaciones.\n",
        "\n",
        "Si dos variables están muy correlacionadas, Naïve Bayes puede “contar la misma evidencia dos veces”, lo que a veces empeora la calibración o el desempeño. Una forma común de mitigar esto es reducir dimensionalidad (por ejemplo, PCA), seleccionar variables o agrupar variables altamente redundantes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23867bf5",
      "metadata": {
        "id": "23867bf5"
      },
      "outputs": [],
      "source": [
        "# Correlación entre variables numéricas (subconjunto)\n",
        "num_cols = [c for c in [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"] if c in df.columns]\n",
        "\n",
        "# Asegurar que TotalCharges sea numérico si existe como texto\n",
        "if \"TotalCharges\" in num_cols:\n",
        "    df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "\n",
        "corr = df[num_cols].corr()\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.imshow(corr, interpolation=\"nearest\")\n",
        "plt.xticks(range(len(num_cols)), num_cols, rotation=45, ha=\"right\")\n",
        "plt.yticks(range(len(num_cols)), num_cols)\n",
        "plt.colorbar()\n",
        "plt.title(\"Correlación entre variables numéricas (subconjunto)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "display(corr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eab7905",
      "metadata": {
        "id": "1eab7905"
      },
      "source": [
        "### 1.6 Resumen (Naïve Bayes)\n",
        "\n",
        "En términos generales, Naïve Bayes destaca por su **rapidez** y por ser un buen punto de comparación inicial. Su principal limitación es que simplifica la relación entre variables, por lo que puede perder rendimiento cuando existen dependencias fuertes o relaciones no lineales relevantes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c161922",
      "metadata": {
        "id": "6c161922"
      },
      "source": [
        "## Segundo paso. Support Vector Machines (SVM)\n",
        "\n",
        "En esta sección se entrenan y comparan dos variantes:\n",
        "- **SVM lineal:** útil cuando buscamos una separación relativamente simple y valoramos interpretabilidad a nivel de pesos (en sentido conceptual).\n",
        "- **SVM con kernel RBF:** útil cuando la relación entre variables y churn es más compleja y no se separa bien con una frontera lineal.\n",
        "\n",
        "En SVM, el **escalamiento** es crítico porque el modelo se basa en distancias. Si una variable tiene valores muy grandes, puede dominar la decisión y distorsionar el resultado. Por eso mantenemos el mismo preprocesamiento con StandardScaler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c01a78f",
      "metadata": {
        "id": "8c01a78f"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# Pipelines SVM (probability=True permite predict_proba; esto tiene costo computacional adicional)\n",
        "pipe_svm_linear = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"clf\", SVC(kernel=\"linear\", probability=True, random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "pipe_svm_rbf = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"clf\", SVC(kernel=\"rbf\", probability=True, random_state=RANDOM_STATE))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bef9d3dd",
      "metadata": {
        "id": "bef9d3dd"
      },
      "source": [
        "### 2.1 Selección de hiperparámetros (Grid Search vs Random Search)\n",
        "\n",
        "Se comparan **GridSearchCV** y **RandomizedSearchCV** siguiendo la lógica de la Actividad 2.  \n",
        "La métrica principal será **PR-AUC** (Average Precision), porque la clase churn suele ser minoritaria y esta métrica refleja mejor el rendimiento sobre esa clase.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503ecbfd",
      "metadata": {
        "id": "503ecbfd"
      },
      "outputs": [],
      "source": [
        "# Grillas / distribuciones\n",
        "param_grid_linear = {\n",
        "    \"clf__C\": [0.1, 1, 10]\n",
        "}\n",
        "\n",
        "param_grid_rbf = {\n",
        "    \"clf__C\": [0.1, 1, 10],\n",
        "    \"clf__gamma\": [0.001, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "# ==============\n",
        "# SVM LINEAL\n",
        "# ==============\n",
        "grid_linear = GridSearchCV(\n",
        "    estimator=pipe_svm_linear,\n",
        "    param_grid=param_grid_linear,\n",
        "    cv=cv5,\n",
        "    scoring=\"average_precision\",\n",
        "    n_jobs=-1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "grid_linear.fit(X_train, y_train)\n",
        "grid_linear_time = time.perf_counter() - t0\n",
        "\n",
        "rand_linear = RandomizedSearchCV(\n",
        "    estimator=pipe_svm_linear,\n",
        "    param_distributions=param_grid_linear,\n",
        "    n_iter=10,\n",
        "    cv=cv5,\n",
        "    scoring=\"average_precision\",\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "rand_linear.fit(X_train, y_train)\n",
        "rand_linear_time = time.perf_counter() - t0\n",
        "\n",
        "print(\"SVM lineal | Grid time:\", round(grid_linear_time, 3), \"s | Best PR-AUC:\", grid_linear.best_score_)\n",
        "print(\"SVM lineal | Rand time:\", round(rand_linear_time, 3), \"s | Best PR-AUC:\", rand_linear.best_score_)\n",
        "\n",
        "best_linear_search = grid_linear if grid_linear.best_score_ >= rand_linear.best_score_ else rand_linear\n",
        "best_svm_linear = best_linear_search.best_estimator_\n",
        "print(\"Mejor búsqueda (lineal):\", type(best_linear_search).__name__)\n",
        "pprint(best_linear_search.best_params_)\n",
        "\n",
        "# ==============\n",
        "# SVM RBF\n",
        "# ==============\n",
        "grid_rbf = GridSearchCV(\n",
        "    estimator=pipe_svm_rbf,\n",
        "    param_grid=param_grid_rbf,\n",
        "    cv=cv5,\n",
        "    scoring=\"average_precision\",\n",
        "    n_jobs=-1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "grid_rbf.fit(X_train, y_train)\n",
        "grid_rbf_time = time.perf_counter() - t0\n",
        "\n",
        "rand_rbf = RandomizedSearchCV(\n",
        "    estimator=pipe_svm_rbf,\n",
        "    param_distributions=param_grid_rbf,\n",
        "    n_iter=12,\n",
        "    cv=cv5,\n",
        "    scoring=\"average_precision\",\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "rand_rbf.fit(X_train, y_train)\n",
        "rand_rbf_time = time.perf_counter() - t0\n",
        "\n",
        "print(\"SVM RBF   | Grid time:\", round(grid_rbf_time, 3), \"s | Best PR-AUC:\", grid_rbf.best_score_)\n",
        "print(\"SVM RBF   | Rand time:\", round(rand_rbf_time, 3), \"s | Best PR-AUC:\", rand_rbf.best_score_)\n",
        "\n",
        "best_rbf_search = grid_rbf if grid_rbf.best_score_ >= rand_rbf.best_score_ else rand_rbf\n",
        "best_svm_rbf = best_rbf_search.best_estimator_\n",
        "print(\"Mejor búsqueda (RBF):\", type(best_rbf_search).__name__)\n",
        "pprint(best_rbf_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15547967",
      "metadata": {
        "id": "15547967"
      },
      "source": [
        "### 2.2 Evaluación final en TEST (SVM lineal y SVM RBF)\n",
        "\n",
        "Se reportan las métricas solicitadas en el conjunto de test y se grafican las curvas ROC y Precision–Recall.  \n",
        "Esto permite comparar desempeño realista y también visualizar el compromiso entre detectar churn y evitar falsas alarmas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b84fae6",
      "metadata": {
        "id": "3b84fae6"
      },
      "outputs": [],
      "source": [
        "def evaluate_on_test(model, X_train, y_train, X_test, y_test, title=\"Modelo\"):\n",
        "    t0 = time.perf_counter()\n",
        "    model.fit(X_train, y_train)\n",
        "    fit_time = time.perf_counter() - t0\n",
        "\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1v = f1_score(y_test, y_pred, zero_division=0)\n",
        "    rocA = roc_auc_score(y_test, y_proba)\n",
        "    prA = average_precision_score(y_test, y_proba)\n",
        "\n",
        "    print(f\"\\n=== {title} (TEST) ===\")\n",
        "    print(\"Tiempo de entrenamiento (aprox.):\", round(fit_time, 3), \"s\")\n",
        "    print(\"Accuracy :\", round(acc, 3))\n",
        "    print(\"Precision:\", round(prec, 3))\n",
        "    print(\"Recall   :\", round(rec, 3))\n",
        "    print(\"F1       :\", round(f1v, 3))\n",
        "    print(\"AUC-ROC  :\", round(rocA, 3))\n",
        "    print(\"PR-AUC   :\", round(prA, 3))\n",
        "\n",
        "    print(\"\\nMatriz de confusión:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    p, r, _ = precision_recall_curve(y_test, y_proba)\n",
        "    ap = average_precision_score(y_test, y_proba)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f\"ROC (AUC={roc_auc:.3f})\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "    plt.title(f\"{title} – Curva ROC (TEST)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(r, p, label=f\"PR (AP={ap:.3f})\")\n",
        "    plt.xlabel(\"Recall\")\n",
        "    plt.ylabel(\"Precision\")\n",
        "    plt.title(f\"{title} – Curva Precision–Recall (TEST)\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return {\n",
        "        \"fit_time_s\": fit_time,\n",
        "        \"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1v, \"roc_auc\": rocA, \"pr_auc\": prA\n",
        "    }\n",
        "\n",
        "metrics_svm_linear = evaluate_on_test(best_svm_linear, X_train, y_train, X_test, y_test, title=\"SVM lineal (mejor)\")\n",
        "metrics_svm_rbf = evaluate_on_test(best_svm_rbf, X_train, y_train, X_test, y_test, title=\"SVM RBF (mejor)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "632a9a44",
      "metadata": {
        "id": "632a9a44"
      },
      "source": [
        "### 2.3 k-fold (curvas promedio)\n",
        "\n",
        "A modo complementario, también evaluamos los mejores SVM con k-fold estratificado para observar estabilidad de métricas y curvas promedio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7279cf5c",
      "metadata": {
        "id": "7279cf5c"
      },
      "outputs": [],
      "source": [
        "df_svm_lin_cv, summary_svm_lin, _ = evaluate_model_cv_detailed(\n",
        "    best_svm_linear, X, y, cv=cv5, plot_curves=True, verbose=True\n",
        ")\n",
        "\n",
        "df_svm_rbf_cv, summary_svm_rbf, _ = evaluate_model_cv_detailed(\n",
        "    best_svm_rbf, X, y, cv=cv5, plot_curves=True, verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a481cad5",
      "metadata": {
        "id": "a481cad5"
      },
      "source": [
        "### 2.4 Desbalance: class_weight='balanced'\n",
        "\n",
        "Se re-entrena el mejor SVM incorporando class_weight='balanced'.  \n",
        "En general, esta opción busca compensar el desbalance dando más “importancia” a la clase minoritaria, lo que suele aumentar recall de churn, aunque puede afectar la precisión.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e371161",
      "metadata": {
        "id": "4e371161"
      },
      "outputs": [],
      "source": [
        "# Aplicar class_weight='balanced' a los mejores modelos\n",
        "best_svm_linear_bal = best_svm_linear.set_params(clf__class_weight=\"balanced\")\n",
        "best_svm_rbf_bal = best_svm_rbf.set_params(clf__class_weight=\"balanced\")\n",
        "\n",
        "metrics_svm_linear_bal = evaluate_on_test(best_svm_linear_bal, X_train, y_train, X_test, y_test, title=\"SVM lineal (balanced)\")\n",
        "metrics_svm_rbf_bal = evaluate_on_test(best_svm_rbf_bal, X_train, y_train, X_test, y_test, title=\"SVM RBF (balanced)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7704834",
      "metadata": {
        "id": "f7704834"
      },
      "source": [
        "## Paso 3. Comparación final y análisis crítico\n",
        "\n",
        "En esta sección se comparan Naïve Bayes, SVM lineal y SVM RBF en tres dimensiones:\n",
        "1) **Desempeño** (en especial F1 y PR-AUC)  \n",
        "2) **Interpretabilidad**  \n",
        "3) **Costo computacional** (tiempos de búsqueda/entrenamiento)\n",
        "\n",
        "Además, se discute por qué el escalamiento es tan importante en SVM y cómo la codificación one-hot afecta la dimensionalidad y el tiempo de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a332b179",
      "metadata": {
        "id": "a332b179"
      },
      "outputs": [],
      "source": [
        "# Tabla comparativa (TEST) – resumen de métricas y tiempos aproximados\n",
        "\n",
        "rows = []\n",
        "\n",
        "# NB (tiempo de búsqueda + métricas test)\n",
        "rows.append({\n",
        "    \"model\": \"Naïve Bayes (GaussianNB)\",\n",
        "    \"search_time_s\": nb_search_time,\n",
        "    \"fit_time_s\": np.nan,\n",
        "    \"accuracy\": accuracy_score(y_test, (y_proba_nb >= 0.5).astype(int)),\n",
        "    \"precision\": precision_score(y_test, (y_proba_nb >= 0.5).astype(int), zero_division=0),\n",
        "    \"recall\": recall_score(y_test, (y_proba_nb >= 0.5).astype(int), zero_division=0),\n",
        "    \"f1\": f1_score(y_test, (y_proba_nb >= 0.5).astype(int), zero_division=0),\n",
        "    \"roc_auc\": roc_auc_score(y_test, y_proba_nb),\n",
        "    \"pr_auc\": average_precision_score(y_test, y_proba_nb)\n",
        "})\n",
        "\n",
        "# SVMs\n",
        "for name, m in [\n",
        "    (\"SVM lineal (mejor)\", metrics_svm_linear),\n",
        "    (\"SVM RBF (mejor)\", metrics_svm_rbf),\n",
        "    (\"SVM lineal (balanced)\", metrics_svm_linear_bal),\n",
        "    (\"SVM RBF (balanced)\", metrics_svm_rbf_bal),\n",
        "]:\n",
        "    rows.append({\n",
        "        \"model\": name,\n",
        "        \"search_time_s\": np.nan,   # búsqueda ya reportada arriba; aquí dejamos como referencia simple\n",
        "        **m\n",
        "    })\n",
        "\n",
        "df_compare = pd.DataFrame(rows)\n",
        "display(df_compare)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a63c5d73",
      "metadata": {
        "id": "a63c5d73"
      },
      "source": [
        "### 3.1 Interpretabilidad, desempeño y costo computacional (síntesis)\n",
        "\n",
        "**Naïve Bayes** suele ser el más rápido y simple, pero su supuesto de independencia puede limitar el desempeño si existen variables redundantes o altamente dependientes.  \n",
        "\n",
        "**SVM lineal** puede ser una buena alternativa cuando la separación entre clases es relativamente simple y valoramos un modelo más “compacto”.  \n",
        "\n",
        "**SVM RBF** suele capturar relaciones más complejas, pero puede ser más costoso en tiempo, especialmente con muchas variables (por ejemplo, por efecto del one-hot encoding).  \n",
        "\n",
        "En problemas de churn, normalmente es útil priorizar métricas enfocadas en la clase positiva (churn), como **PR-AUC** y **F1**, y luego ajustar el umbral o el balance de clases según la capacidad del equipo de retención.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbe68ab4",
      "metadata": {
        "id": "bbe68ab4"
      },
      "source": [
        "### 3.2 ¿Por qué el escalamiento es crítico en SVM?\n",
        "\n",
        "SVM se apoya en distancias y en la forma geométrica de los datos. Cuando una variable tiene una escala muy grande, puede dominar esas distancias y hacer que el modelo “preste atención” casi exclusivamente a esa variable, aunque no sea la más relevante. El escalamiento evita ese problema, equilibrando el aporte relativo de cada feature y facilitando que el modelo encuentre una frontera de separación más representativa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8871b865",
      "metadata": {
        "id": "8871b865"
      },
      "source": [
        "### 3.3 Efecto del one-hot encoding (dimensionalidad y tiempo)\n",
        "\n",
        "La codificación one-hot transforma variables categóricas en muchas columnas binarias. Esto puede aumentar fuertemente la dimensionalidad, lo que tiene dos efectos principales:  \n",
        "1) Puede mejorar separabilidad en algunos casos, porque el modelo ve información más detallada.  \n",
        "2) Puede aumentar el tiempo de entrenamiento, especialmente en métodos como SVM con kernel (RBF), que se vuelven más exigentes cuando crece el número de variables y el tamaño de la matriz.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f14e7a5d",
      "metadata": {
        "id": "f14e7a5d"
      },
      "source": [
        "## Conclusiones\n",
        "\n",
        "- Se implementó Naïve Bayes con selección de hiperparámetros y evaluación robusta, destacando su rapidez y sus limitaciones cuando hay dependencia entre variables.  \n",
        "- Se entrenaron y optimizaron SVM lineal y SVM RBF, comparando Grid Search y Random Search en tiempo y desempeño.  \n",
        "- Se observó que el escalamiento es un requisito práctico para SVM y que el one-hot encoding puede aumentar significativamente la dimensionalidad y el costo computacional.  \n",
        "- En un escenario de retención, la elección final del modelo debe considerar el equilibrio entre detectar churn (recall) y no saturar al equipo con falsos positivos (precision), apoyándose en PR-AUC, F1 y las curvas Precision–Recall.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}