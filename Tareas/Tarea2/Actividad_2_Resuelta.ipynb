{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e16faac3",
      "metadata": {
        "id": "e16faac3"
      },
      "source": [
        "# Actividad 2 – Machine Learning II  \n",
        "## Modelos no lineales para Churn: Árbol de Decisión y Random Forest\n",
        "\n",
        "**Contexto:** en la Actividad 1 se trabajó churn con modelos lineales (Regresión Logística).  \n",
        "En esta Actividad 2 se extiende el análisis a modelos basados en árboles, con foco en:\n",
        "\n",
        "- Búsqueda sistemática de hiperparámetros con GridSearchCV y RandomizedSearchCV.\n",
        "- Comparación de desempeño y costo computacional.\n",
        "- Análisis de varianza/estabilidad de predicciones en Random Forest al variar el número de árboles.\n",
        "- Evaluación con métricas y curvas adecuadas para desbalance de clases (ROC-AUC, PR-AUC, F1).\n",
        "\n",
        "Se mantiene el mismo preprocesamiento\n",
        " usado en la Actividad 1 (imputación + one-hot + escalamiento).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67bf4ac8",
      "metadata": {
        "id": "67bf4ac8"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 0) Librerías\n",
        "# =========================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from pprint import pprint\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_curve,\n",
        "    auc,\n",
        "    precision_recall_curve,\n",
        "    average_precision_score\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "655a421b",
      "metadata": {
        "id": "655a421b"
      },
      "source": [
        "## Primer Paso. Dataset y variables\n",
        "\n",
        "Se utiliza el dataset data-churn.csv (mismo del ejercicio anterior).  \n",
        "La variable objetivo Churn se transforma a binaria: **Yes → 1**, **No → 0**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d94b00c",
      "metadata": {
        "id": "9d94b00c"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 1) Carga del dataset\n",
        "# =========================\n",
        "df = pd.read_csv(\"data-churn.csv\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41a3eb4",
      "metadata": {
        "id": "a41a3eb4"
      },
      "outputs": [],
      "source": [
        "# Variable objetivo binaria\n",
        "df[\"Churn\"] = df[\"Churn\"].map({\"Yes\": 1, \"No\": 0})\n",
        "\n",
        "print(\"Tamaño del dataset:\", df.shape)\n",
        "print(\"\\nProporción de clases (Churn):\")\n",
        "display(df[\"Churn\"].value_counts(normalize=True).rename(\"proporción\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35077fd0",
      "metadata": {
        "id": "35077fd0"
      },
      "outputs": [],
      "source": [
        "# Revisión rápida de faltantes\n",
        "display(df.isna().sum().sort_values(ascending=False).head(15))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4981130b",
      "metadata": {
        "id": "4981130b"
      },
      "source": [
        "## Paso 1. Preprocesamiento (con el enfoque de la Actividad 1)\n",
        "\n",
        "Decisiones principales:\n",
        "- **Faltantes:** imputación simple (mediana en numéricas, más frecuente en categóricas).\n",
        "- **Categóricas:** One-Hot Encoding con drop='first' para evitar multicolinealidad perfecta.\n",
        "- **Numéricas:** estandarización con StandardScaler.\n",
        "\n",
        "Aunque los árboles no requieren escalamiento, se mantiene el mismo preprocesamiento por consistencia con la Actividad 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19ba4efb",
      "metadata": {
        "id": "19ba4efb"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 2) Features y target\n",
        "# =========================\n",
        "target = \"Churn\"\n",
        "\n",
        "X = df.drop(columns=[target]).copy()\n",
        "y = df[target].copy()\n",
        "\n",
        "# Forzar numéricas a tipo numérico (por ejemplo TotalCharges puede venir como texto)\n",
        "for col in X.columns:\n",
        "    if X[col].dtype == \"object\":\n",
        "        # Intentar convertir a numérico si aplica; si no, queda categórica.\n",
        "        pass\n",
        "\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print(\"Numéricas:\", numeric_features)\n",
        "print(\"Categóricas:\", categorical_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33e05a21",
      "metadata": {
        "id": "33e05a21"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 3) Preprocesador (Actividad 1)\n",
        "# =========================\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer, numeric_features),\n",
        "        (\"cat\", categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "cv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cf104a5",
      "metadata": {
        "id": "5cf104a5"
      },
      "source": [
        "## Funciones auxiliares\n",
        "Se definen funciones para:\n",
        "- Obtener nombres de features tras el preprocesamiento.\n",
        "- Evaluar modelos con k-fold estratificado (métricas + curvas ROC/PR promedio).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97baf9a0",
      "metadata": {
        "id": "97baf9a0"
      },
      "outputs": [],
      "source": [
        "def get_feature_names_from_preprocessor(preprocessor: ColumnTransformer):\n",
        "    \"\"\"Devuelve nombres de columnas luego del ColumnTransformer.\"\"\"\n",
        "    feature_names = []\n",
        "    for name, transformer, cols in preprocessor.transformers_:\n",
        "        if name == \"remainder\" and transformer == \"drop\":\n",
        "            continue\n",
        "        if hasattr(transformer, \"named_steps\"):\n",
        "            # Pipeline\n",
        "            last_step = list(transformer.named_steps.values())[-1]\n",
        "            if hasattr(last_step, \"get_feature_names_out\"):\n",
        "                try:\n",
        "                    names = last_step.get_feature_names_out(cols)\n",
        "                except:\n",
        "                    names = last_step.get_feature_names_out()\n",
        "                feature_names.extend(names)\n",
        "            else:\n",
        "                feature_names.extend(cols)\n",
        "        else:\n",
        "            if hasattr(transformer, \"get_feature_names_out\"):\n",
        "                try:\n",
        "                    names = transformer.get_feature_names_out(cols)\n",
        "                except:\n",
        "                    names = transformer.get_feature_names_out()\n",
        "                feature_names.extend(names)\n",
        "            else:\n",
        "                feature_names.extend(cols)\n",
        "    return [str(n) for n in feature_names]\n",
        "\n",
        "\n",
        "def evaluate_model_cv(model, X, y, cv, title=\"Modelo\", plot_curves=True, verbose=True):\n",
        "    \"\"\"Evalúa un modelo con StratifiedKFold: métricas por fold + resumen + curvas promedio.\"\"\"\n",
        "    per_fold = []\n",
        "    roc_curves = []\n",
        "    pr_curves = []\n",
        "\n",
        "    mean_fpr = np.linspace(0, 1, 200)\n",
        "    mean_recall = np.linspace(0, 1, 200)\n",
        "\n",
        "    y_true_all, y_pred_all = [], []\n",
        "    # (Para PR-AUC usamos average_precision_score, que trabaja con probabilidades)\n",
        "    y_proba_all = []\n",
        "\n",
        "    for fold, (tr, te) in enumerate(cv.split(X, y), start=1):\n",
        "        X_tr, X_te = X.iloc[tr], X.iloc[te]\n",
        "        y_tr, y_te = y.iloc[tr], y.iloc[te]\n",
        "\n",
        "        model.fit(X_tr, y_tr)\n",
        "        y_pred = model.predict(X_te)\n",
        "        y_proba = model.predict_proba(X_te)[:, 1]\n",
        "\n",
        "        acc = accuracy_score(y_te, y_pred)\n",
        "        prec = precision_score(y_te, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_te, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_te, y_pred, zero_division=0)\n",
        "        pr_auc = average_precision_score(y_te, y_proba)\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_te, y_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        per_fold.append({\n",
        "            \"fold\": fold,\n",
        "            \"accuracy\": acc,\n",
        "            \"precision\": prec,\n",
        "            \"recall\": rec,\n",
        "            \"f1\": f1,\n",
        "            \"roc_auc\": roc_auc,\n",
        "            \"pr_auc\": pr_auc\n",
        "        })\n",
        "\n",
        "        # Curvas interpoladas (promedio)\n",
        "        tpr_i = np.interp(mean_fpr, fpr, tpr)\n",
        "        tpr_i[0] = 0.0\n",
        "        roc_curves.append(tpr_i)\n",
        "\n",
        "        p, r, _ = precision_recall_curve(y_te, y_proba)\n",
        "        order = np.argsort(r)\n",
        "        r_sorted = r[order]\n",
        "        p_sorted = p[order]\n",
        "        p_i = np.interp(mean_recall, r_sorted, p_sorted)\n",
        "        pr_curves.append(p_i)\n",
        "\n",
        "        y_true_all.extend(y_te)\n",
        "        y_pred_all.extend(y_pred)\n",
        "        y_proba_all.extend(y_proba)\n",
        "\n",
        "    df_fold = pd.DataFrame(per_fold)\n",
        "    summary = pd.DataFrame({\n",
        "        \"mean\": df_fold.drop(columns=[\"fold\"]).mean(),\n",
        "        \"std\": df_fold.drop(columns=[\"fold\"]).std()\n",
        "    })\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n=== {title} ===\")\n",
        "        display(df_fold)\n",
        "        display(summary)\n",
        "\n",
        "        cm = confusion_matrix(y_true_all, y_pred_all)\n",
        "        print(\"Matriz de confusión global (todos los folds):\")\n",
        "        print(cm)\n",
        "        print(\"\\nReporte de clasificación global:\")\n",
        "        print(classification_report(y_true_all, y_pred_all, digits=3))\n",
        "\n",
        "    curves = None\n",
        "    if plot_curves:\n",
        "        mean_tpr = np.mean(roc_curves, axis=0)\n",
        "        mean_tpr[-1] = 1.0\n",
        "        mean_roc_auc = auc(mean_fpr, mean_tpr)\n",
        "\n",
        "        mean_precision = np.mean(pr_curves, axis=0)\n",
        "        mean_pr_auc = auc(mean_recall, mean_precision)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(mean_fpr, mean_tpr, label=f\"ROC promedio (AUC≈{mean_roc_auc:.3f})\")\n",
        "        plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "        plt.xlabel(\"False Positive Rate\")\n",
        "        plt.ylabel(\"True Positive Rate (Recall)\")\n",
        "        plt.title(f\"Curva ROC promedio – {title}\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(mean_recall, mean_precision, label=f\"PR promedio (AUC≈{mean_pr_auc:.3f})\")\n",
        "        plt.xlabel(\"Recall\")\n",
        "        plt.ylabel(\"Precision\")\n",
        "        plt.title(f\"Curva Precision–Recall promedio – {title}\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        curves = {\n",
        "            \"mean_fpr\": mean_fpr,\n",
        "            \"mean_tpr\": mean_tpr,\n",
        "            \"mean_recall\": mean_recall,\n",
        "            \"mean_precision\": mean_precision\n",
        "        }\n",
        "\n",
        "    return df_fold, summary, curves\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1cb0a04",
      "metadata": {
        "id": "f1cb0a04"
      },
      "source": [
        "# Segundo paso. Árbol de decisión (Grid Search vs Random Search)\n",
        "\n",
        "Se ajusta un DecisionTreeClassifier utilizando el mismo preprocesamiento de la Actividad 1.\n",
        "\n",
        "## 1.1 Grilla de hiperparámetros y justificación\n",
        "- **max_depth:** controla la profundidad del árbol. Valores altos aumentan riesgo de sobreajuste.\n",
        "- **min_samples_split / min_samples_leaf:** evitan particiones demasiado específicas.\n",
        "- **criterion:** cambia la forma de medir la “calidad” de una división (gini/entropy/log_loss).\n",
        "\n",
        "Se utiliza como métrica principal PR-AUC (Average Precision), adecuada cuando `churn=1` es minoritario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2636020",
      "metadata": {
        "id": "c2636020"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# 4) Árbol de decisión + búsqueda de hiperparámetros\n",
        "# =========================\n",
        "pipe_dt = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"clf\", DecisionTreeClassifier(random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "param_grid_dt = {\n",
        "    \"clf__max_depth\": [None, 3, 5, 8, 12, 16],\n",
        "    \"clf__min_samples_split\": [2, 10, 25, 50],\n",
        "    \"clf__min_samples_leaf\": [1, 5, 10, 25],\n",
        "    \"clf__criterion\": [\"gini\", \"entropy\", \"log_loss\"]\n",
        "}\n",
        "\n",
        "scoring = \"average_precision\"  # PR-AUC\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af14d9e3",
      "metadata": {
        "id": "af14d9e3"
      },
      "outputs": [],
      "source": [
        "# 1) Grid Search CV\n",
        "grid_dt = GridSearchCV(\n",
        "    estimator=pipe_dt,\n",
        "    param_grid=param_grid_dt,\n",
        "    cv=cv5,\n",
        "    scoring=scoring,\n",
        "    n_jobs=-1,\n",
        "    refit=True,\n",
        "    return_train_score=False\n",
        ")\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "grid_dt.fit(X, y)\n",
        "grid_time = time.perf_counter() - t0\n",
        "\n",
        "print(\"GridSearchCV time (s):\", round(grid_time, 3))\n",
        "print(\"Best params (Grid):\")\n",
        "pprint(grid_dt.best_params_)\n",
        "print(\"Best score (Grid) [PR-AUC]:\", grid_dt.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4198c0c1",
      "metadata": {
        "id": "4198c0c1"
      },
      "outputs": [],
      "source": [
        "# 2) Random Search CV\n",
        "rand_dt = RandomizedSearchCV(\n",
        "    estimator=pipe_dt,\n",
        "    param_distributions=param_grid_dt,\n",
        "    n_iter=50,                   # balance entre cobertura y tiempo\n",
        "    cv=cv5,\n",
        "    scoring=scoring,\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    refit=True,\n",
        "    return_train_score=False\n",
        ")\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "rand_dt.fit(X, y)\n",
        "rand_time = time.perf_counter() - t0\n",
        "\n",
        "print(\"RandomizedSearchCV time (s):\", round(rand_time, 3))\n",
        "print(\"Best params (Random):\")\n",
        "pprint(rand_dt.best_params_)\n",
        "print(\"Best score (Random) [PR-AUC]:\", rand_dt.best_score_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25ebc7d0",
      "metadata": {
        "id": "25ebc7d0"
      },
      "outputs": [],
      "source": [
        "# Comparación directa: tiempo y mejor score\n",
        "comparison_search = pd.DataFrame({\n",
        "    \"method\": [\"GridSearchCV\", \"RandomizedSearchCV\"],\n",
        "    \"time_seconds\": [grid_time, rand_time],\n",
        "    \"best_pr_auc\": [grid_dt.best_score_, rand_dt.best_score_]\n",
        "})\n",
        "\n",
        "display(comparison_search)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "877b0edc",
      "metadata": {
        "id": "877b0edc"
      },
      "outputs": [],
      "source": [
        "# Selección del mejor árbol (por PR-AUC; en empate, el más rápido)\n",
        "if rand_dt.best_score_ > grid_dt.best_score_:\n",
        "    best_dt_search = rand_dt\n",
        "elif grid_dt.best_score_ > rand_dt.best_score_:\n",
        "    best_dt_search = grid_dt\n",
        "else:\n",
        "    # Empate: elegir el método más rápido\n",
        "    best_dt_search = rand_dt if rand_time < grid_time else grid_dt\n",
        "\n",
        "best_dt = best_dt_search.best_estimator_\n",
        "\n",
        "print(\"Método elegido:\", type(best_dt_search).__name__)\n",
        "print(\"Mejores hiperparámetros:\")\n",
        "pprint(best_dt_search.best_params_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb44c241",
      "metadata": {
        "id": "cb44c241"
      },
      "source": [
        "## 1.2 Evaluación del mejor Árbol (k-fold)\n",
        "\n",
        "Se evalúa el árbol seleccionado con k-fold estratificado reportando métricas y curvas ROC/PR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214733cc",
      "metadata": {
        "id": "214733cc"
      },
      "outputs": [],
      "source": [
        "df_dt, summary_dt, curves_dt = evaluate_model_cv(\n",
        "    best_dt, X, y, cv=cv5, title=\"Decision Tree óptimo\", plot_curves=True, verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00a5ad52",
      "metadata": {
        "id": "00a5ad52"
      },
      "source": [
        "# Tercer paso. Visualización e interpretación del árbol óptimo\n",
        "\n",
        "Se entrena el árbol óptimo en todo el dataset para visualizar sus primeras divisiones.\n",
        "Para mejorar legibilidad, se limita la profundidad visualizada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "125254fe",
      "metadata": {
        "id": "125254fe"
      },
      "outputs": [],
      "source": [
        "# Entrenar el mejor árbol en todo el dataset para visualizarlo\n",
        "best_dt.fit(X, y)\n",
        "\n",
        "dt_clf = best_dt.named_steps[\"clf\"]\n",
        "feature_names = get_feature_names_from_preprocessor(best_dt.named_steps[\"preprocess\"])\n",
        "\n",
        "print(\"Total features tras preprocesamiento:\", len(feature_names))\n",
        "print(\"Ejemplos:\", feature_names[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a86ac0bb",
      "metadata": {
        "id": "a86ac0bb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 10))\n",
        "plot_tree(\n",
        "    dt_clf,\n",
        "    feature_names=feature_names,\n",
        "    class_names=[\"No churn (0)\", \"Churn (1)\"],\n",
        "    filled=True,\n",
        "    max_depth=2,            # ajustar si necesitas más/menos detalle\n",
        "    impurity=False,\n",
        "    proportion=True,\n",
        "    rounded=True,\n",
        "    fontsize=8\n",
        ")\n",
        "plt.title(\"Árbol de decisión óptimo (profundidad visual limitada)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9d4c440",
      "metadata": {
        "id": "a9d4c440"
      },
      "source": [
        "**Interpretación (guía breve):**\n",
        "- Las variables que aparecen en los **primeros nodos** suelen ser las más influyentes en la decisión del árbol.\n",
        "- A diferencia de la regresión logística (coeficientes globales), el árbol ofrece reglas tipo *“si… entonces…”*,\n",
        "  lo que mejora la interpretabilidad a nivel de reglas, pero puede ser inestable si el árbol es muy profundo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68a72d2d",
      "metadata": {
        "id": "68a72d2d"
      },
      "source": [
        "# Cuarto paso. Random Forest y análisis de varianza (número de árboles)\n",
        "\n",
        "Se estudia la estabilidad de un Random Forest al variar el número de árboles:\n",
        "\n",
        "n_estimators = [2, 4, 8, 16, 32, 64, 128]\n",
        "\n",
        "Para cada configuración:\n",
        "- Se entrena con k-fold estratificado.\n",
        "- Se registra el desempeño (**F1, ROC-AUC, PR-AUC**).\n",
        "- Se estima la **varianza de las probabilidades predichas entre folds** de esta forma:\n",
        "  1) para cada fold, se entrena un modelo con el set de entrenamiento del fold;  \n",
        "  2) ese modelo predice probabilidades sobre **todo X**;  \n",
        "  3) se calcula la varianza por muestra entre los modelos de folds y se reporta el promedio (estabilidad global).\n",
        "\n",
        "Esto refleja cuán sensibles son las predicciones a cambios en los datos de entrenamiento.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afde604e",
      "metadata": {
        "id": "afde604e"
      },
      "outputs": [],
      "source": [
        "def rf_variance_study(X, y, cv, n_estimators_list, rf_params=None):\n",
        "    if rf_params is None:\n",
        "        rf_params = {}\n",
        "\n",
        "    rows = []\n",
        "    for n_estimators in n_estimators_list:\n",
        "        fold_models_proba_full = []  # proba sobre todo X por cada fold-model\n",
        "        fold_metrics = []\n",
        "        total_fit_time = 0.0\n",
        "\n",
        "        for tr, te in cv.split(X, y):\n",
        "            X_tr, X_te = X.iloc[tr], X.iloc[te]\n",
        "            y_tr, y_te = y.iloc[tr], y.iloc[te]\n",
        "\n",
        "            model = Pipeline(steps=[\n",
        "                (\"preprocess\", preprocessor),\n",
        "                (\"clf\", RandomForestClassifier(\n",
        "                    n_estimators=n_estimators,\n",
        "                    random_state=RANDOM_STATE,\n",
        "                    n_jobs=-1,\n",
        "                    **rf_params\n",
        "                ))\n",
        "            ])\n",
        "\n",
        "            t0 = time.perf_counter()\n",
        "            model.fit(X_tr, y_tr)\n",
        "            total_fit_time += (time.perf_counter() - t0)\n",
        "\n",
        "            # Predicciones en fold (métricas)\n",
        "            y_proba = model.predict_proba(X_te)[:, 1]\n",
        "            y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "            f1 = f1_score(y_te, y_pred, zero_division=0)\n",
        "            pr_auc = average_precision_score(y_te, y_proba)\n",
        "\n",
        "            fpr, tpr, _ = roc_curve(y_te, y_proba)\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "\n",
        "            fold_metrics.append((f1, roc_auc, pr_auc))\n",
        "\n",
        "            # Probabilidades sobre todo X (para varianza entre folds)\n",
        "            proba_full = model.predict_proba(X)[:, 1]\n",
        "            fold_models_proba_full.append(proba_full)\n",
        "\n",
        "        fold_models_proba_full = np.vstack(fold_models_proba_full)  # shape: (n_folds, n_samples)\n",
        "        var_per_sample = np.var(fold_models_proba_full, axis=0)     # varianza entre folds por muestra\n",
        "        mean_var = float(np.mean(var_per_sample))\n",
        "\n",
        "        fold_metrics = np.array(fold_metrics)  # (n_folds, 3)\n",
        "        rows.append({\n",
        "            \"n_estimators\": n_estimators,\n",
        "            \"mean_variance_proba\": mean_var,\n",
        "            \"f1_mean\": fold_metrics[:, 0].mean(),\n",
        "            \"roc_auc_mean\": fold_metrics[:, 1].mean(),\n",
        "            \"pr_auc_mean\": fold_metrics[:, 2].mean(),\n",
        "            \"fit_time_total_seconds\": total_fit_time\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "n_estimators_list = [2, 4, 8, 16, 32, 64, 128]\n",
        "df_var = rf_variance_study(X, y, cv=cv5, n_estimators_list=n_estimators_list, rf_params={\"max_features\": \"sqrt\"})\n",
        "display(df_var)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a8cdd78",
      "metadata": {
        "id": "2a8cdd78"
      },
      "outputs": [],
      "source": [
        "# Gráfico: varianza vs número de árboles\n",
        "plt.figure()\n",
        "plt.plot(df_var[\"n_estimators\"], df_var[\"mean_variance_proba\"], marker=\"o\")\n",
        "plt.xscale(\"log\", base=2)\n",
        "plt.xlabel(\"Número de árboles (n_estimators)\")\n",
        "plt.ylabel(\"Varianza promedio de probabilidades (entre folds)\")\n",
        "plt.title(\"Estabilidad de predicciones: Varianza vs número de árboles\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f27dcb8",
      "metadata": {
        "id": "0f27dcb8"
      },
      "outputs": [],
      "source": [
        "# Gráfico: métricas vs número de árboles\n",
        "plt.figure()\n",
        "plt.plot(df_var[\"n_estimators\"], df_var[\"f1_mean\"], marker=\"o\", label=\"F1\")\n",
        "plt.plot(df_var[\"n_estimators\"], df_var[\"roc_auc_mean\"], marker=\"o\", label=\"ROC-AUC\")\n",
        "plt.plot(df_var[\"n_estimators\"], df_var[\"pr_auc_mean\"], marker=\"o\", label=\"PR-AUC\")\n",
        "plt.xscale(\"log\", base=2)\n",
        "plt.xlabel(\"Número de árboles (n_estimators)\")\n",
        "plt.ylabel(\"Métrica promedio (k-fold)\")\n",
        "plt.title(\"Desempeño vs número de árboles\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dd92941",
      "metadata": {
        "id": "5dd92941"
      },
      "source": [
        "**Discusión esperada (guía):**\n",
        "- Al aumentar n_estimators, es común observar reducción de varianza (mayor estabilidad).\n",
        "- El desempeño puede mejorar hasta cierto punto y luego estabilizarse.\n",
        "- El costo computacional aumenta (tiempo total de entrenamiento).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a676fc2",
      "metadata": {
        "id": "0a676fc2"
      },
      "source": [
        "# Quinto paso. Selección del mejor Random Forest (búsqueda de hiperparámetros)\n",
        "\n",
        "Se define una grilla/distribuciones de hiperparámetros y se utiliza RandomizedSearchCV\n",
        "para equilibrar cobertura y tiempo computacional.  \n",
        "La métrica principal sigue siendo PR-AUC (Average Precision).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45889354",
      "metadata": {
        "id": "45889354"
      },
      "outputs": [],
      "source": [
        "pipe_rf = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"clf\", RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))\n",
        "])\n",
        "\n",
        "param_dist_rf = {\n",
        "    \"clf__n_estimators\": [64, 128, 256],\n",
        "    \"clf__max_depth\": [None, 5, 10, 15],\n",
        "    \"clf__min_samples_leaf\": [1, 5, 10],\n",
        "    \"clf__max_features\": [\"sqrt\", \"log2\", None],\n",
        "    \"clf__class_weight\": [None, \"balanced\"]\n",
        "}\n",
        "\n",
        "rand_rf = RandomizedSearchCV(\n",
        "    estimator=pipe_rf,\n",
        "    param_distributions=param_dist_rf,\n",
        "    n_iter=40,\n",
        "    cv=cv5,\n",
        "    scoring=\"average_precision\",\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    refit=True,\n",
        "    return_train_score=False\n",
        ")\n",
        "\n",
        "t0 = time.perf_counter()\n",
        "rand_rf.fit(X, y)\n",
        "rf_search_time = time.perf_counter() - t0\n",
        "\n",
        "print(\"RandomizedSearchCV RF time (s):\", round(rf_search_time, 3))\n",
        "print(\"Best params (RF):\")\n",
        "pprint(rand_rf.best_params_)\n",
        "print(\"Best score (RF) [PR-AUC]:\", rand_rf.best_score_)\n",
        "\n",
        "best_rf = rand_rf.best_estimator_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3368e2d0",
      "metadata": {
        "id": "3368e2d0"
      },
      "source": [
        "## 4.1 Evaluación del mejor Random Forest (k-fold)\n",
        "Se evalúa el Random Forest seleccionado, reportando métricas y curvas ROC/PR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ccbacf",
      "metadata": {
        "id": "60ccbacf"
      },
      "outputs": [],
      "source": [
        "df_rf, summary_rf, curves_rf = evaluate_model_cv(\n",
        "    best_rf, X, y, cv=cv5, title=\"Random Forest óptimo\", plot_curves=True, verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0338ff3",
      "metadata": {
        "id": "c0338ff3"
      },
      "source": [
        "# Sexto paso. Comparación final y mejora con pesos por clase\n",
        "\n",
        "Según la pauta:\n",
        "- Se intenta mejorar usando class_weight=\"balanced\".\n",
        "- Se comparan Árbol óptimo vs Random Forest óptimo.\n",
        "- Se reportan métricas (Accuracy, Precision, Recall, F1) y curvas ROC/PR.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bcd07e6",
      "metadata": {
        "id": "3bcd07e6"
      },
      "outputs": [],
      "source": [
        "# Mejorar Árbol con class_weight (si no estaba ya considerado)\n",
        "dt_balanced = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"clf\", DecisionTreeClassifier(\n",
        "        random_state=RANDOM_STATE,\n",
        "        class_weight=\"balanced\",\n",
        "        **{k.replace(\"clf__\", \"\"): v for k, v in best_dt_search.best_params_.items()}\n",
        "    ))\n",
        "])\n",
        "\n",
        "df_dt_b, summary_dt_b, _ = evaluate_model_cv(\n",
        "    dt_balanced, X, y, cv=cv5, title=\"Decision Tree (class_weight='balanced')\", plot_curves=False, verbose=False\n",
        ")\n",
        "\n",
        "# Mejorar RF con class_weight (si no estaba ya)\n",
        "best_rf_params = best_rf.named_steps[\"clf\"].get_params()\n",
        "best_rf_params.update({\"class_weight\": \"balanced\"})\n",
        "\n",
        "rf_balanced = Pipeline(steps=[\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"clf\", RandomForestClassifier(**best_rf_params))\n",
        "])\n",
        "\n",
        "df_rf_b, summary_rf_b, _ = evaluate_model_cv(\n",
        "    rf_balanced, X, y, cv=cv5, title=\"Random Forest (class_weight='balanced')\", plot_curves=False, verbose=False\n",
        ")\n",
        "\n",
        "display(summary_dt_b)\n",
        "display(summary_rf_b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d32daa47",
      "metadata": {
        "id": "d32daa47"
      },
      "outputs": [],
      "source": [
        "# Tabla comparativa final (promedios)\n",
        "def summarize_for_compare(summary, model_name):\n",
        "    out = summary[\"mean\"].copy()\n",
        "    out.name = model_name\n",
        "    return out\n",
        "\n",
        "compare = pd.concat([\n",
        "    summarize_for_compare(summary_dt, \"Decision Tree\"),\n",
        "    summarize_for_compare(summary_rf, \"Random Forest\"),\n",
        "    summarize_for_compare(summary_dt_b, \"Decision Tree (balanced)\"),\n",
        "    summarize_for_compare(summary_rf_b, \"Random Forest (balanced)\"),\n",
        "], axis=1)\n",
        "\n",
        "display(compare)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4731466",
      "metadata": {
        "id": "f4731466"
      },
      "source": [
        "# Séptimo paso. Respuestas a preguntas de la pauta\n",
        "\n",
        "**Relación entre varianza, ensambles y capacidad de generalización**\n",
        "\n",
        "Random Forest se basa en la combinación de múltiples árboles de decisión entrenados de forma aleatoria, tanto en la selección de observaciones (bootstrap) como en el subconjunto de variables utilizadas en cada división. Este enfoque permite reducir la varianza respecto a un árbol individual, haciendo que el modelo sea menos sensible a cambios en los datos de entrenamiento y, en consecuencia, mejore su estabilidad y capacidad de generalización.\n",
        "\n",
        "**Ventajas y desventajas de árboles de decisión frente a modelos lineales**\n",
        "\n",
        "Los árboles de decisión tienen la ventaja de capturar relaciones no lineales y generar reglas del tipo “si–entonces”, lo que facilita su interpretación, especialmente cuando el árbol es poco profundo. Sin embargo, pueden ser propensos al sobreajuste.\n",
        "En contraste, los modelos lineales, como la regresión logística, son conceptualmente más simples y ofrecen una interpretación global más directa a través de sus coeficientes, aunque pueden resultar limitados cuando las relaciones entre variables y la respuesta son complejas o no lineales.\n",
        "\n",
        "**¿Cuándo preferir un árbol interpretable en lugar de un Random Forest?**\n",
        "\n",
        "Un árbol de decisión interpretable puede ser preferible cuando la transparencia del modelo es prioritaria, por ejemplo, en contextos de auditoría, cumplimiento normativo o comunicación con áreas no técnicas del negocio. En estos casos, se acepta un posible sacrificio en desempeño predictivo a cambio de una explicación clara y comprensible de las decisiones del modelo.\n",
        "\n",
        "**Conexión con la teoría: reducción de varianza mediante ensambles**\n",
        "\n",
        "Desde un punto de vista teórico, los métodos de ensamble reducen la varianza al promediar las predicciones de múltiples modelos individuales, lo que atenúa errores específicos de cada uno. Esta idea se refleja en los resultados empíricos obtenidos, donde se observa una disminución de la varianza de las predicciones a medida que aumenta el número de árboles en el Random Forest.\n",
        "\n",
        "**Métrica prioritaria desde la perspectiva del negocio (retención de clientes)**\n",
        "\n",
        "En campañas de retención, resulta especialmente relevante minimizar la cantidad de clientes que abandonan el servicio sin ser detectados por el modelo. Por este motivo, métricas como Recall, así como métricas orientadas a la clase positiva como PR-AUC y F1, suelen ser más adecuadas que la accuracy. No obstante, la selección final debe considerar la capacidad operativa del equipo de retención, evitando generar un volumen excesivo de falsos positivos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c895cd16",
      "metadata": {
        "id": "c895cd16"
      },
      "source": [
        "## Conclusiones\n",
        "- Se seleccionó un árbol de decisión mediante Grid/Random Search y se compararon tiempos y desempeño.\n",
        "- Se visualizó el árbol óptimo y se discutió su interpretabilidad.\n",
        "- En Random Forest, al aumentar el número de árboles se observó una tendencia a mayor estabilidad (menor varianza),\n",
        "  con un costo computacional mayor.\n",
        "- La comparación final sugiere que Random Forest suele superar a un árbol individual por su capacidad de generalización,\n",
        "  especialmente en problemas con ruido y desbalance como churn.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}